# Documentation du Pipeline IDS Kafka - Phase 1 Compl√©t√©e

## Vue d'Ensemble

Ce document explique l'architecture du pipeline de d√©tection d'intrusions en temps r√©el impl√©ment√© avec Apache Kafka, le mod√®le AutoencoderIDS, et pr√©pare la phase 2 (int√©gration LLM).

---

## Architecture Actuelle (Phase 1)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    PIPELINE IDS KAFKA - PHASE 1                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Dataset CSV (CICIDS2017)
    ‚îÇ
    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ TrafficProducer  ‚îÇ  ‚Üí Simule le trafic r√©seau
‚îÇ  (G√©n√©rateur)    ‚îÇ     Lit depuis cicids2017_cleaned.csv
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ produce
         ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ ids-raw-data    ‚îÇ  Topic Kafka (3 partitions)
    ‚îÇ  (Topic)        ‚îÇ  Format: {flow_id, features[37], label, timestamp}
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ consume
             ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ DataPreprocessor ‚îÇ  ‚Üí Valide et transforme les donn√©es
    ‚îÇ  (Consumer/      ‚îÇ     Normalisation, validation
    ‚îÇ   Producer)      ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ produce
             ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ ids-features    ‚îÇ  Topic Kafka (3 partitions)
    ‚îÇ  (Topic)        ‚îÇ  Format: {flow_id, features, label, preprocessed_at}
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ consume
             ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ ThreatDetector   ‚îÇ  ‚Üí D√©tection avec AutoencoderIDS
    ‚îÇ  (Consumer/      ‚îÇ     Pr√©diction + Calcul de s√©v√©rit√©
    ‚îÇ   Producer)      ‚îÇ     PUBLIE UNIQUEMENT SI ATTAQUE
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ produce (si attaque)
             ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ ids-alerts      ‚îÇ  Topic Kafka (1 partition)
    ‚îÇ  (Topic)        ‚îÇ  Format: {alert_type, confidence, severity, ...}
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ consume
             ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ AlertMonitor     ‚îÇ  ‚Üí Affichage console avec couleurs
    ‚îÇ  (Consumer)      ‚îÇ     Statistiques en temps r√©el
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

    Phase 2 (√Ä IMPL√âMENTER)
             ‚îÇ
             ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ LLMExplainer     ‚îÇ  ‚Üí G√©n√®re explications avec LLM
    ‚îÇ  (Consumer/      ‚îÇ     Consomme depuis ids-alerts
    ‚îÇ   Producer)      ‚îÇ     Produit vers ids-explanations
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ produce
             ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇids-explanations ‚îÇ  Topic Kafka (1 partition)
    ‚îÇ  (Topic)        ‚îÇ  Format: {explanation_text, analysis, ...}
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Composants du Syst√®me

### 1. **TrafficProducer** (Simulateur de Trafic)

**R√¥le** : G√©n√®re des flux r√©seau depuis le dataset CICIDS2017 et les envoie vers Kafka.

**Fonctionnement** :
- Lit le fichier CSV `cicids2017_cleaned.csv`
- S√©lectionne al√©atoirement des flux (normal + attaques)
- Respecte le ratio d'attaques sp√©cifi√© (`--attack-ratio`)
- Ajoute un d√©lai configurable entre les messages (`--delay`)

**Topic de sortie** : `ids-raw-data`

**Format des messages** :
```json
{
  "flow_id": "sim_00000042",
  "features": [80.0, 0.0, 2.0, 128.0, ...],  // 37 features
  "label": "DDoS",
  "timestamp": 1703672400.0
}
```

**Code cl√©** :
```python
for i, flow in enumerate(self.simulator.generate_stream(count, attack_ratio)):
    self.producer.send('ids-raw-data', flow.to_dict())
    time.sleep(delay)
```

---

### 2. **DataPreprocessor** (Pr√©processeur)

**R√¥le** : Valide et transforme les donn√©es brutes avant la d√©tection.

**Fonctionnement** :
- Consomme depuis `ids-raw-data`
- Valide la structure des donn√©es
- Ajoute des m√©tadonn√©es (timestamp de pr√©processing)
- Produit vers `ids-features`

**Topics** :
- **Input** : `ids-raw-data`
- **Output** : `ids-features`

**Format de sortie** :
```json
{
  "flow_id": "sim_00000042",
  "features": [80.0, 0.0, 2.0, ...],
  "label": "DDoS",
  "timestamp": 1703672400.0,
  "preprocessed_at": "2024-12-28T10:45:23.456789"
}
```

**Am√©liorations possibles** :
- Validation des valeurs (min/max, NaN)
- Normalisation suppl√©mentaire
- Feature engineering

---

### 3. **ThreatDetector** (D√©tecteur de Menaces)

**R√¥le** : Applique le mod√®le AutoencoderIDS et **publie uniquement les alertes d'attaques**.

**Fonctionnement** :
1. Consomme depuis `ids-features`
2. Charge le mod√®le PyTorch `autoencoder_ids_v1.1.0.pt`
3. Pr√©dit la classe (7 types : Bots, Brute Force, DDoS, DoS, Normal, Port Scanning, Web Attacks)
4. Calcule la confiance et le score d'anomalie
5. D√©termine la s√©v√©rit√© (CRITIQUE, √âLEV√âE, MOYENNE, FAIBLE)
6. **PUBLIE VERS `ids-alerts` UNIQUEMENT SI ATTAQUE D√âTECT√âE**

**Topics** :
- **Input** : `ids-features`
- **Output** : `ids-alerts` (UNIQUEMENT pour les attaques)

**Format de sortie** :
```json
{
  "timestamp": "2024-12-28T10:45:23.456789",
  "flow_id": "sim_00000042",
  "alert_type": "DDoS",
  "confidence": 0.9534,
  "anomaly_score": 0.002341,
  "severity": "CRITIQUE",
  "true_label": "DDoS",
  "correct": true,
  "all_probabilities": {
    "Bots": 0.0012,
    "Brute Force": 0.0023,
    "DDoS": 0.9534,
    "DoS": 0.0312,
    "Normal Traffic": 0.0089,
    "Port Scanning": 0.0015,
    "Web Attacks": 0.0015
  },
  "top_3_classes": [
    ["DDoS", 0.9534],
    ["DoS", 0.0312],
    ["Normal Traffic", 0.0089]
  ],
  "features_summary": {
    "preprocessed_at": "2024-12-28T10:45:23.123456",
    "original_timestamp": 1703672400.0
  }
}
```

**Calcul de la s√©v√©rit√©** :
```python
def _calculate_severity(self, prediction) -> str:
    if prediction.confidence >= 0.9:
        return "CRITIQUE"      # 90%+ confiance
    elif prediction.confidence >= 0.7:
        return "√âLEV√âE"        # 70-90% confiance
    elif prediction.confidence >= 0.5:
        return "MOYENNE"       # 50-70% confiance
    else:
        return "FAIBLE"        # <50% confiance
```

**IMPORTANT** :
- Si le trafic est Normal, aucun message n'est envoy√© vers `ids-alerts`
- Seules les attaques d√©tect√©es g√©n√®rent des alertes
- Les explications seront g√©n√©r√©es par le LLMExplainer (Phase 2)

---

### 4. **AlertMonitor** (Moniteur d'Alertes)

**R√¥le** : Affiche les alertes en temps r√©el dans la console avec formatage et statistiques.

**Fonctionnement** :
- Consomme depuis `ids-alerts`
- Affiche chaque alerte avec code couleur selon la s√©v√©rit√©
- Calcule des statistiques en temps r√©el
- Affiche un rapport final √† l'arr√™t (Ctrl+C)

**Topic d'entr√©e** : `ids-alerts`

**Affichage console** :
```
======================================================================
üö® ALERTE S√âCURIT√â - S√©v√©rit√©: CRITIQUE
======================================================================
  Horodatage:     2024-12-28T10:45:23.456789
  Flow ID:        sim_00000042
  Type d'attaque: DDoS
  Confiance:      94.7%
  Score anomalie: 0.002341
  Label r√©el:     DDoS
  Pr√©diction:     ‚úì CORRECTE
======================================================================
```

**Codes couleur** :
- Rouge : CRITIQUE (confiance ‚â•90%)
- Jaune : √âLEV√âE (confiance 70-90%)
- Bleu : MOYENNE (confiance 50-70%)
- Vert : FAIBLE (confiance <50%)

**Statistiques finales** :
```
STATISTIQUES DE SURVEILLANCE
======================================================================
  Dur√©e:          102.3s
  Total alertes:  287
  Taux:           2.81 alertes/s

  R√©partition par type:
    - DDoS                :  112 (39.0%)
    - DoS                 :   78 (27.2%)
    - Port Scanning       :   45 (15.7%)
    - Web Attacks         :   32 (11.1%)
    - Brute Force         :   15 (5.2%)
    - Bots                :    5 (1.7%)
======================================================================
```

---

## Flux de Donn√©es D√©taill√©

### √âtape 1 : G√©n√©ration du Trafic
```
TrafficProducer
    ‚Üì
Lit cicids2017_cleaned.csv (500 flux)
    ‚Üì
S√©lectionne al√©atoirement selon attack_ratio
    ‚Üì
Envoie vers ids-raw-data avec d√©lai
```

### √âtape 2 : Pr√©processing
```
DataPreprocessor consomme ids-raw-data
    ‚Üì
Valide la structure
    ‚Üì
Ajoute timestamp de preprocessing
    ‚Üì
Produit vers ids-features
```

### √âtape 3 : D√©tection
```
ThreatDetector consomme ids-features
    ‚Üì
Charge features dans le mod√®le AutoencoderIDS
    ‚Üì
Pr√©diction : classe + confiance + anomaly_score
    ‚Üì
Calcule la s√©v√©rit√©
    ‚Üì
SI attaque ‚Üí Envoie vers ids-alerts
SI normal  ‚Üí Rien (log console uniquement)
```

### √âtape 4 : Monitoring
```
AlertMonitor consomme ids-alerts
    ‚Üì
Affiche avec code couleur
    ‚Üì
Met √† jour les statistiques
    ‚Üì
Ctrl+C ‚Üí Affiche rapport final
```

---

## Utilisation

### Commandes Principales

```bash
# Test rapide (200 flux, 20% attaques, d√©lai 0.05s)
python automated_ids_pipeline.py --count 200 --attack-ratio 0.2 --delay 0.05

# Test standard (1000 flux, 30% attaques)
python automated_ids_pipeline.py --count 1000 --attack-ratio 0.3 --delay 0.1

# Test intensif (5000 flux, 50% attaques, rapide)
python automated_ids_pipeline.py --count 5000 --attack-ratio 0.5 --delay 0.01

# Sp√©cifier le serveur Kafka
python automated_ids_pipeline.py --count 500 --kafka-server localhost:9092
```

### Arguments Disponibles

| Argument | Type | D√©faut | Description |
|----------|------|--------|-------------|
| `--count` | int | 1000 | Nombre de flux √† traiter |
| `--attack-ratio` | float | 0.3 | Proportion d'attaques (0.0-1.0) |
| `--delay` | float | 0.1 | D√©lai entre messages (secondes) |
| `--dataset` | str | ../dataset/cicids2017_cleaned.csv | Chemin du dataset |
| `--kafka-server` | str | localhost:9093 | Adresse Kafka |

---

## Topics Kafka

| Topic | Partitions | R√¥le | Format |
|-------|-----------|------|--------|
| **ids-raw-data** | 3 | Donn√©es brutes du simulateur | {flow_id, features, label, timestamp} |
| **ids-features** | 3 | Features pr√©process√©es | {flow_id, features, label, preprocessed_at} |
| **ids-alerts** | 1 | Alertes d'attaques uniquement | {alert_type, confidence, severity, ...} |
| **ids-explanations** | 1 | Explications LLM (Phase 2) | {explanation_text, analysis, ...} |

---

## Ce que fait le fichier `automated_ids_pipeline.py`

### 1. **Architecture Multi-Thread**

Le script utilise le **threading Python** pour ex√©cuter simultan√©ment :
- DataPreprocessor (thread daemon)
- ThreatDetector (thread daemon)
- AlertMonitor (thread daemon)
- TrafficProducer (thread principal)

**Pourquoi des threads ?**
- Permet le traitement en **temps r√©el**
- Chaque composant fonctionne **ind√©pendamment**
- Simule un v√©ritable syst√®me distribu√©

```python
# Lancement des threads
t1 = threading.Thread(target=preprocessor.run, daemon=True)
t2 = threading.Thread(target=detector.run, daemon=True)
t3 = threading.Thread(target=monitor.run, daemon=True)

t1.start()
t2.start()
t3.start()
```

### 2. **Pattern Producer-Consumer de Kafka**

Chaque composant impl√©mente le pattern **Producer-Consumer** :

```python
class DataPreprocessor:
    def __init__(self):
        # Consumer pour lire
        self.consumer = KafkaConsumer('ids-raw-data', ...)
        
        # Producer pour √©crire
        self.producer = KafkaProducer(...)
    
    def run(self):
        for message in self.consumer:
            # Traiter
            processed = self.process_flow(message.value)
            
            # Publier
            self.producer.send('ids-features', processed)
```

### 3. **Gestion des Erreurs et Arr√™t Propre**

```python
try:
    for message in self.consumer:
        # Traitement
        pass
except KeyboardInterrupt:
    print("Arr√™t demand√©")
finally:
    self.consumer.close()
    self.producer.close()
```

### 4. **S√©rialisation JSON**

Tous les messages Kafka sont s√©rialis√©s en JSON :

```python
KafkaProducer(
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

KafkaConsumer(
    value_deserializer=lambda m: json.loads(m.decode('utf-8'))
)
```

---

## Phase 2 : Int√©gration LLM (√Ä FAIRE)

### Objectif

Cr√©er un composant **LLMExplainer** qui :
1. **Consomme** depuis `ids-alerts`
2. **G√©n√®re des explications** en langage naturel avec un LLM
3. **Produit** vers `ids-explanations`

### Architecture Cible

```
ids-alerts (alertes brutes)
    ‚Üì
LLMExplainer
    ‚îÇ
    ‚îú‚îÄ Lit l'alerte
    ‚îú‚îÄ Extrait les features importantes
    ‚îú‚îÄ Appelle un LLM (GPT, Claude, Llama, etc.)
    ‚îú‚îÄ G√©n√®re explication d√©taill√©e
    ‚îÇ
    ‚Üì
ids-explanations (explications en langage naturel)
```

### Format de Sortie Attendu (`ids-explanations`)

```json
{
  "timestamp": "2024-12-28T10:45:25.789012",
  "flow_id": "sim_00000042",
  "alert_reference": {
    "alert_type": "DDoS",
    "confidence": 0.9534,
    "severity": "CRITIQUE"
  },
  "explanation": {
    "summary": "Une attaque DDoS a √©t√© d√©tect√©e avec une confiance tr√®s √©lev√©e de 95.3%.",
    "technical_analysis": "Le mod√®le a identifi√© des patterns caract√©ristiques d'une attaque DDoS : volume de trafic anormal, taux de paquets √©lev√©, et signatures r√©seau typiques d'un flood de requ√™tes.",
    "why_detected": "Les 3 indicateurs principaux sont : 1) Taux de paquets/seconde 10x sup√©rieur √† la normale, 2) Dur√©e de connexion extr√™mement courte, 3) Distribution anormale des ports sources.",
    "risk_assessment": "Risque critique. Cette attaque pourrait saturer les ressources r√©seau et rendre les services indisponibles.",
    "recommended_actions": [
      "Bloquer imm√©diatement l'adresse IP source",
      "Activer les r√®gles de rate limiting",
      "Notifier l'√©quipe SOC",
      "V√©rifier la disponibilit√© des services critiques"
    ]
  },
  "llm_metadata": {
    "model": "claude-3-sonnet",
    "tokens_used": 245,
    "generation_time_ms": 1250
  }
}
```

---

## üìù T√¢ches pour le Prochain Membre de l'√âquipe (Phase 2)

### Mission : Impl√©menter le Composant LLMExplainer

Vous devez cr√©er un nouveau fichier `llm_explainer.py` qui :

### 1. **Classe LLMExplainer**

```python
class LLMExplainer:
    """G√©n√®re des explications d√©taill√©es des alertes avec un LLM"""
    
    def __init__(self, llm_config: dict, bootstrap_servers: str):
        # Consumer depuis ids-alerts
        self.consumer = KafkaConsumer('ids-alerts', ...)
        
        # Producer vers ids-explanations
        self.producer = KafkaProducer(...)
        
        # Client LLM (OpenAI, Anthropic, Hugging Face, etc.)
        self.llm_client = self._init_llm(llm_config)
    
    def generate_explanation(self, alert: dict) -> dict:
        """G√©n√®re une explication avec le LLM"""
        # 1. Construire le prompt
        prompt = self._build_prompt(alert)
        
        # 2. Appeler le LLM
        response = self.llm_client.generate(prompt)
        
        # 3. Parser la r√©ponse
        explanation = self._parse_llm_response(response)
        
        return explanation
    
    def run(self):
        """Boucle principale"""
        for message in self.consumer:
            alert = message.value
            
            # G√©n√©rer explication
            explanation = self.generate_explanation(alert)
            
            # Publier
            self.producer.send('ids-explanations', explanation)
```

### 2. **Construction du Prompt LLM**

Cr√©ez une fonction qui transforme l'alerte en prompt structur√© :

```python
def _build_prompt(self, alert: dict) -> str:
    return f"""
Tu es un expert en cybers√©curit√©. Analyse cette alerte IDS et g√©n√®re une explication d√©taill√©e.

ALERTE:
- Type: {alert['alert_type']}
- Confiance: {alert['confidence']:.1%}
- S√©v√©rit√©: {alert['severity']}
- Score d'anomalie: {alert['anomaly_score']:.6f}

PROBABILIT√âS:
{self._format_probabilities(alert['all_probabilities'])}

CONTEXTE:
- Flow ID: {alert['flow_id']}
- Timestamp: {alert['timestamp']}

G√âN√àRE:
1. Un r√©sum√© en une phrase
2. Une analyse technique d√©taill√©e
3. Pourquoi cette attaque a √©t√© d√©tect√©e
4. √âvaluation du risque
5. Actions recommand√©es (liste de 3-5 actions)

R√©ponds en JSON avec la structure suivante:
{{
  "summary": "...",
  "technical_analysis": "...",
  "why_detected": "...",
  "risk_assessment": "...",
  "recommended_actions": ["action1", "action2", ...]
}}
"""
```

### 3. **Int√©gration LLM**

Choisissez **un** LLM parmi :

#### Option A : **OpenAI GPT** (Recommand√©)
```python
from openai import OpenAI

client = OpenAI(api_key="votre-cl√©")

response = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": prompt}],
    temperature=0.3
)

explanation = json.loads(response.choices[0].message.content)
```

#### Option B : **Anthropic Claude**
```python
from anthropic import Anthropic

client = Anthropic(api_key="votre-cl√©")

response = client.messages.create(
    model="claude-3-sonnet-20240229",
    max_tokens=1024,
    messages=[{"role": "user", "content": prompt}]
)

explanation = json.loads(response.content[0].text)
```

#### Option C : **Hugging Face (Llama, Mistral, etc.)**
```python
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf")
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-chat-hf")

inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=512)
explanation = tokenizer.decode(outputs[0])
```

#### Option D : **Ollama (Local)**
```python
import requests

response = requests.post('http://localhost:11434/api/generate', json={
    'model': 'llama2',
    'prompt': prompt,
    'stream': False
})

explanation = json.loads(response.json()['response'])
```

### 4. **Fichier de Configuration**

Cr√©ez llm_config.json :

```json
{
  "provider": "openai",
  "model": "gpt-4",
  "api_key_env": "OPENAI_API_KEY",
  "temperature": 0.3,
  "max_tokens": 1024,
  "timeout": 30
}
```

### 5. **Tests**

Cr√©ez test_llm_explainer.py :

```python
def test_explanation_generation():
    """Test la g√©n√©ration d'explication"""
    explainer = LLMExplainer(config)
    
    # Alerte de test
    alert = {
        'alert_type': 'DDoS',
        'confidence': 0.95,
        'severity': 'CRITIQUE',
        'anomaly_score': 0.002341,
        'all_probabilities': {...}
    }
    
    # G√©n√©rer explication
    explanation = explainer.generate_explanation(alert)
    
    # V√©rifications
    assert 'summary' in explanation
    assert 'technical_analysis' in explanation
    assert len(explanation['recommended_actions']) >= 3
    
    print("‚úì Test r√©ussi")
```

### 6. **Int√©gration dans le Pipeline**

Modifiez automated_ids_pipeline.py :

```python
from llm_explainer import LLMExplainer

def run_pipeline(...):
    # ...composants existants...
    
    # 4. LLMExplainer (Phase 2)
    explainer = LLMExplainer(llm_config, kafka_server)
    t4 = threading.Thread(target=explainer.run, daemon=True)
    t4.start()
    threads.append(t4)
    
    # ...reste du code...
```

---

## Diagramme de S√©quence Complet (Phase 1 + Phase 2)

```
TrafficProducer     DataPreprocessor    ThreatDetector      AlertMonitor      LLMExplainer
       ‚îÇ                   ‚îÇ                   ‚îÇ                   ‚îÇ                 ‚îÇ
       ‚îÇ‚îÄ‚îÄ‚îÄ flow ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ                   ‚îÇ                   ‚îÇ                 ‚îÇ
       ‚îÇ                   ‚îÇ                   ‚îÇ                   ‚îÇ                 ‚îÇ
       ‚îÇ                   ‚îÇ‚îÄ‚îÄ‚îÄ features ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ                   ‚îÇ                 ‚îÇ
       ‚îÇ                   ‚îÇ                   ‚îÇ                   ‚îÇ                 ‚îÇ
       ‚îÇ                   ‚îÇ                   ‚îÇ‚îÄ predict()        ‚îÇ                 ‚îÇ
       ‚îÇ                   ‚îÇ                   ‚îÇ                   ‚îÇ                 ‚îÇ
       ‚îÇ                   ‚îÇ                   ‚îÇ‚îÄ‚îÄ‚îÄ alert ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ                 ‚îÇ
       ‚îÇ                   ‚îÇ                   ‚îÇ                   ‚îÇ                 ‚îÇ
       ‚îÇ                   ‚îÇ                   ‚îÇ                   ‚îÇ‚îÄ display        ‚îÇ
       ‚îÇ                   ‚îÇ                   ‚îÇ                   ‚îÇ                 ‚îÇ
       ‚îÇ                   ‚îÇ                   ‚îÇ‚îÄ‚îÄ‚îÄ alert ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ
       ‚îÇ                   ‚îÇ                   ‚îÇ                   ‚îÇ                 ‚îÇ
       ‚îÇ                   ‚îÇ                   ‚îÇ                   ‚îÇ                 ‚îÇ‚îÄ LLM call
       ‚îÇ                   ‚îÇ                   ‚îÇ                   ‚îÇ                 ‚îÇ
       ‚îÇ                   ‚îÇ                   ‚îÇ                   ‚îÇ‚óÑ‚îÄ explanation ‚îÄ‚îÄ‚îÇ
       ‚îÇ                   ‚îÇ                   ‚îÇ                   ‚îÇ                 ‚îÇ
```

---

## Crit√®res de Succ√®s (Phase 2)

LLMExplainer consomme correctement depuis ids-alerts
G√©n√®re des explications coh√©rentes et d√©taill√©es
Produit vers ids-explanations avec format JSON valide
G√®re les erreurs LLM (timeout, quota, etc.)
Affiche les explications dans la console ou dashboard
Tests unitaires passent
Documentation mise √† jour

---

## D√©pendances Suppl√©mentaires (Phase 2)

Ajoutez √† `requirements.txt` :

```txt
# Existant
kafka-python>=2.0.2
torch>=2.0.0
numpy>=1.24.0
pandas>=2.0.0
scikit-learn>=1.3.0
joblib>=1.3.0

# Phase 2 - LLM
openai>=1.0.0              # Si OpenAI
anthropic>=0.18.0          # Si Claude
transformers>=4.30.0       # Si Hugging Face
torch>=2.0.0               # Si mod√®les locaux
requests>=2.31.0           # Si Ollama
```

---

## Contact et Support

Pour toute question sur l'impl√©mentation :
- Consulter le code existant dans `automated_ids_pipeline.py`
- Lire la doc Kafka : https://kafka.apache.org/documentation/
- Exemples LLM : Voir section "Int√©gration LLM" ci-dessus

**Bonne chance pour la Phase 2 !**